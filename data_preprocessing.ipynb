{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import UCF101\n",
    "import torchvision.io as io\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms.functional as F\n",
    "import random\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucf101_root = \"./Datasets/UCF101/UCF-101\"  \n",
    "ucf101_annot = \"E:/sai/UCE/Video_summarization/Datasets/UCF101/UCF101TrainTestSplits/UCF_annotations\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transform (resize, augmentations, etc.)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize frames to 224x224\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),  # Normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 833/833 [04:45<00:00,  2.92it/s]\n",
      "100%|██████████| 833/833 [04:08<00:00,  3.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# UCF101 dataset using the prebuilt class in torchvision\n",
    "train_dataset = UCF101(root=ucf101_root, annotation_path=ucf101_annot, train=True, transform=transform, frames_per_clip=16)\n",
    "test_dataset = UCF101(root=ucf101_root, annotation_path=ucf101_annot, train=False, transform=transform, frames_per_clip=16)\n",
    "\n",
    "# Split train dataset into train/val sets\n",
    "train_idx, val_idx = train_test_split(list(range(len(train_dataset))), test_size=0.2, random_state=42)\n",
    "train_split = torch.utils.data.Subset(train_dataset, train_idx)\n",
    "val_split = torch.utils.data.Subset(train_dataset, val_idx)\n",
    "\n",
    "# DataLoader for batch processing\n",
    "train_loader = DataLoader(train_split, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_split, batch_size=8, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define augmentation for individual frames using PyTorch\n",
    "def augment_frame_pytorch(frame):\n",
    "    # Example augmentation: Random horizontal flip\n",
    "    if random.random() > 0.5:\n",
    "        frame = F.hflip(frame)\n",
    "    return frame\n",
    "\n",
    "# Function to apply preprocessing on the videos using GPU\n",
    "def process_videos_pytorch(loader):\n",
    "    for batch_idx, (videos, labels) in enumerate(loader):\n",
    "        videos = videos.to(device)  # Transfer videos to GPU\n",
    "        labels = labels.to(device)\n",
    "        # Apply frame-level augmentations here (optional)\n",
    "        augmented_videos = [augment_frame_pytorch(video) for video in videos]\n",
    "        # You can now pass the augmented_videos to your model\n",
    "        print(f\"Processed batch {batch_idx+1}/{len(loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training dataset...\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Process the training dataset\n",
    "print(\"Processing training dataset...\")\n",
    "process_videos_pytorch(train_loader)\n",
    "\n",
    "# Step 2: Process the validation dataset\n",
    "print(\"Processing validation dataset...\")\n",
    "process_videos_pytorch(val_loader)\n",
    "\n",
    "# Step 3: Process the test dataset\n",
    "print(\"Processing test dataset...\")\n",
    "process_videos_pytorch(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # Remove ToTensor if the input is already a tensor\n",
    "    # transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),  # Resizing frames\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __getitem__(self, idx):\n",
    "    video = self.dataset[idx]  # Load the video\n",
    "    print(type(video))  # Check if it's already a tensor or ndarray\n",
    "    video = self.transform(video)\n",
    "    return video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
